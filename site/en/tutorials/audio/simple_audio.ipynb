{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fluF3_oOgkWF"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "cellView": "form",
        "id": "AJs7HHFmg1M9"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYysdyb-CaWM"
      },
      "source": [
        "# Simple audio recognition: Recognizing keywords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNbqmZy0gbyE"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/audio/simple_audio\">\n",
        "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
        "    View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/audio/simple_audio.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
        "    Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/audio/simple_audio.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
        "    View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/audio/simple_audio.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPfDNFlb66XF"
      },
      "source": [
        "This tutorial demonstrates how to preprocess audio files in the WAV format and build and train a basic [automatic speech recognition](https://en.wikipedia.org/wiki/Speech_recognition) (ASR) model for recognizing ten different words. You will use a portion of the [Speech Commands dataset](https://www.tensorflow.org/datasets/catalog/speech_commands) ([Warden, 2018](https://arxiv.org/abs/1804.03209)), which contains short (one-second or less) audio clips of commands, such as \"down\", \"go\", \"left\", \"no\", \"right\", \"stop\", \"up\" and \"yes\".\n",
        "\n",
        "Real-world speech and audio recognition [systems](https://ai.googleblog.com/search/label/Speech%20Recognition) are complex. But, like [image classification with the MNIST dataset](../quickstart/beginner.ipynb), this tutorial should give you a basic understanding of the techniques involved."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Go9C3uLL8Izc"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Import necessary modules and dependencies. You'll be using `tf.keras.utils.audio_dataset_from_directory` (introduced in TensorFlow 2.10), which helps generate audio classification datasets from directories of `.wav` files. You'll also need [seaborn](https://seaborn.pydata.org) for visualization in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "hhNW45sjDEDe"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q tensorflow tensorflow_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "dzLKpmZICaWN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pathlib\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "from IPython import display\n",
        "\n",
        "# Set the seed value for experiment reproducibility.\n",
        "seed = 42\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR0EdgrLCaWR"
      },
      "source": [
        "## Import the mini Speech Commands dataset\n",
        "\n",
        "To save time with data loading, you will be working with a smaller version of the Speech Commands dataset. The [original dataset](https://www.tensorflow.org/datasets/catalog/speech_commands) consists of over 105,000 audio files in the [WAV (Waveform) audio file format](https://www.aelius.com/njh/wavemetatools/doc/riffmci.pdf) of people saying 35 different words. This data was collected by Google and released under a CC BY license.\n",
        "\n",
        "Download and extract the `mini_speech_commands.zip` file containing the smaller Speech Commands datasets with `tf.keras.utils.get_file`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "2-rayb7-3Y0I",
        "outputId": "845538d5-6b63-442e-d989-d38ac9335df8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://raw.githubusercontent.com/gaby-1998/birdguard/refs/heads/main/data/dataset.zip\n",
            "\u001b[1m48567536/48567536\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "DATASET_PATH = 'data/dataset/'\n",
        "\n",
        "data_dir = pathlib.Path(DATASET_PATH)\n",
        "if not data_dir.exists():\n",
        "  tf.keras.utils.get_file(\n",
        "      'dataset.zip',\n",
        "      origin=\"https://raw.githubusercontent.com/gaby-1998/birdguard/refs/heads/main/data/dataset.zip\",\n",
        "      extract=True,\n",
        "      cache_dir='.', cache_subdir='data')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgvFq3uYiS5G"
      },
      "source": [
        "The dataset's audio clips are stored in eight folders corresponding to each speech command: `no`, `yes`, `down`, `go`, `left`, `up`, `right`, and `stop`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "70IBxSKxA1N9",
        "outputId": "c267bb27-9dfc-4ddb-cd80-df0e7b2e1b13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "Could not find directory data/dataset",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-5c3ad82a2720>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcommands\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcommands\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcommands\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommands\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'README.md'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcommands\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'.DS_Store'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Commands:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mlist_directory_v2\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    766\u001b[0m   \"\"\"\n\u001b[1;32m    767\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m     raise errors.NotFoundError(\n\u001b[0m\u001b[1;32m    769\u001b[0m         \u001b[0mnode_def\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0mop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: Could not find directory data/dataset"
          ]
        }
      ],
      "source": [
        "commands = np.array(tf.io.gfile.listdir(str(data_dir)))\n",
        "commands = commands[(commands != 'README.md') & (commands != '.DS_Store')]\n",
        "print('Commands:', commands)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZ7GJjDvHqtt"
      },
      "source": [
        "Divided into directories this way, you can easily load the data using `keras.utils.audio_dataset_from_directory`.\n",
        "\n",
        "The audio clips are 1 second or less at 16kHz. The `output_sequence_length=16000` pads the short ones to exactly 1 second (and would trim longer ones) so that they can be easily batched."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFM4c3aMC8Qv"
      },
      "outputs": [],
      "source": [
        "train_ds, val_ds = tf.keras.utils.audio_dataset_from_directory(\n",
        "    directory=data_dir,\n",
        "    batch_size=64,\n",
        "    validation_split=0.2,\n",
        "    seed=0,\n",
        "    output_sequence_length=44100,\n",
        "    subset='both')\n",
        "\n",
        "label_names = np.array(train_ds.class_names)\n",
        "print()\n",
        "print(\"label names:\", label_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cestp83qFnU5"
      },
      "source": [
        "The dataset now contains batches of audio clips and integer labels. The audio clips have a shape of `(batch, samples, channels)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "3yU6SQGIFb3H",
        "outputId": "6641bdd3-4df6-4b6c-d6a8-53a0a94ba378",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorSpec(shape=(None, 16000), dtype=tf.float32, name=None),\n",
              " TensorSpec(shape=(None,), dtype=tf.int32, name=None))"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "train_ds.element_spec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppG9Dgq2Ex8R"
      },
      "source": [
        "This dataset only contains single channel audio, so use the `tf.squeeze` function to drop the extra axis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "Xl-tnniUIBlM",
        "outputId": "fbe0783d-e1a3-4c73-d991-583006236dcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"<ipython-input-61-6673390826b7>\", line 2, in squeeze  *\n        audio = tf.squeeze(audio, axis=-1)\n\n    ValueError: Can not squeeze dim[1], expected a dimension of 1, got 16000 for '{{node Squeeze}} = Squeeze[T=DT_FLOAT, squeeze_dims=[-1]](args_0)' with input shapes: [?,16000].\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-6673390826b7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mval_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, synchronous, use_unbounded_threadpool, name)\u001b[0m\n\u001b[1;32m   2339\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmap_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2341\u001b[0;31m     return map_op._map_v2(\n\u001b[0m\u001b[1;32m   2342\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2343\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/map_op.py\u001b[0m in \u001b[0;36m_map_v2\u001b[0;34m(input_dataset, map_func, num_parallel_calls, deterministic, synchronous, use_unbounded_threadpool, name)\u001b[0m\n\u001b[1;32m     55\u001b[0m           \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m       )\n\u001b[0;32m---> 57\u001b[0;31m     return _ParallelMapDataset(\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/map_op.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, use_unbounded_threadpool, name)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_inter_op_parallelism\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_inter_op_parallelism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m     self._map_func = structured_function.StructuredFunctionWrapper(\n\u001b[0m\u001b[1;32m    203\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/structured_function.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mfn_factory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace_tf_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefun_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     \u001b[0;31m# There is no graph to add in eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0madd_to_graph\u001b[0m \u001b[0;34m&=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1249\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n\u001b[0;32m-> 1251\u001b[0;31m                          \" decorated with tf.function.\")\n\u001b[0m\u001b[1;32m   1252\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mconcrete\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m     \u001b[0mRaises\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1221\u001b[0;31m       \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mhas\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0myet\u001b[0m \u001b[0mbeen\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0mon\u001b[0m \u001b[0mconcrete\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1222\u001b[0m     \"\"\"\n\u001b[1;32m   1223\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    694\u001b[0m     )\n\u001b[1;32m    695\u001b[0m     \u001b[0;31m# Force the definition of the function for these arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m     self._concrete_variable_creation_fn = tracing_compilation.trace_function(\n\u001b[0m\u001b[1;32m    697\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mtrace_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     concrete_function = _maybe_define_function(\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m           \u001b[0mtarget_func_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlookup_func_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         concrete_function = _create_concrete_function(\n\u001b[0m\u001b[1;32m    284\u001b[0m             \u001b[0mtarget_func_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlookup_func_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36m_create_concrete_function\u001b[0;34m(function_type, type_context, func_graph, tracing_options)\u001b[0m\n\u001b[1;32m    308\u001b[0m       \u001b[0mattributes_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLE_ACD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m   )\n\u001b[0;32m--> 310\u001b[0;31m   traced_func_graph = func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m    311\u001b[0m       \u001b[0mtracing_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m       \u001b[0mtracing_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[1;32m   1057\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1059\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1060\u001b[0m     \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/structured_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    229\u001b[0m       \u001b[0;31m# Note: wrapper_helper will apply autograph based on context.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/structured_function.py\u001b[0m in \u001b[0;36mwrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    159\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_should_unpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariable_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_variables_to_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0m_should_pack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    691\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    688\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/__autograph_generated_filexn6u9e_n.py\u001b[0m in \u001b[0;36mtf__squeeze\u001b[0;34m(audio, labels)\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefinedReturnValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                 \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    329\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_in_allowlist_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Allowlisted %s: from cache'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/weak_tensor_ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_auto_dtype_conversion_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0mbound_arguments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mbound_arguments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs, op_def, extract_traceback)\u001b[0m\n\u001b[1;32m   1054\u001b[0m                                            serialized)\n\u001b[1;32m   1055\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"<ipython-input-61-6673390826b7>\", line 2, in squeeze  *\n        audio = tf.squeeze(audio, axis=-1)\n\n    ValueError: Can not squeeze dim[1], expected a dimension of 1, got 16000 for '{{node Squeeze}} = Squeeze[T=DT_FLOAT, squeeze_dims=[-1]](args_0)' with input shapes: [?,16000].\n"
          ]
        }
      ],
      "source": [
        "def squeeze(audio, labels):\n",
        "  audio = tf.squeeze(audio, axis=-1)\n",
        "  return audio, labels\n",
        "\n",
        "train_ds = train_ds.map(squeeze, tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.map(squeeze, tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtsCSWZN5ILv"
      },
      "source": [
        "The `utils.audio_dataset_from_directory` function only returns up to two splits. It's a good idea to keep a test set separate from your validation set.\n",
        "Ideally you'd keep it in a separate directory, but in this case you can use `Dataset.shard` to split the validation set into two halves. Note that iterating over **any** shard will load **all** the data, and only keep its fraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5UEGsqM5Gss"
      },
      "outputs": [],
      "source": [
        "test_ds = val_ds.shard(num_shards=2, index=0)\n",
        "val_ds = val_ds.shard(num_shards=2, index=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIeoJcwJH5h9"
      },
      "outputs": [],
      "source": [
        "for example_audio, example_labels in train_ds.take(1):\n",
        "  print(example_audio.shape)\n",
        "  print(example_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voxGEwvuh2L7"
      },
      "source": [
        "Let's plot a few audio waveforms:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "dYtGq2zYNHuT",
        "outputId": "da9242df-070a-4da2-e241-edbac762e50c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "index 1 is out of bounds for axis 0 with size 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-f08632cceece>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlabel_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
          ]
        }
      ],
      "source": [
        "label_names[[1,1]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "8yuX6Nqzf6wT",
        "outputId": "9b605ebf-00dc-4d00-97f1-811a08cc9105",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'example_audio' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-94cd9049bcc3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0maudio_signal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexample_audio\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_signal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexample_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'example_audio' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x1000 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.figure(figsize=(16, 10))\n",
        "rows = 3\n",
        "cols = 3\n",
        "n = rows * cols\n",
        "for i in range(n):\n",
        "  plt.subplot(rows, cols, i+1)\n",
        "  audio_signal = example_audio[i]\n",
        "  plt.plot(audio_signal)\n",
        "  plt.title(label_names[example_labels[i]])\n",
        "  plt.yticks(np.arange(-1.2, 1.2, 0.2))\n",
        "  plt.ylim([-1.1, 1.1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWXPphxm0B4m"
      },
      "source": [
        "## Convert waveforms to spectrograms\n",
        "\n",
        "The waveforms in the dataset are represented in the time domain. Next, you'll transform the waveforms from the time-domain signals into the time-frequency-domain signals by computing the [short-time Fourier transform (STFT)](https://en.wikipedia.org/wiki/Short-time_Fourier_transform) to convert the waveforms to as [spectrograms](https://en.wikipedia.org/wiki/Spectrogram), which show frequency changes over time and can be represented as 2D images. You will feed the spectrogram images into your neural network to train the model.\n",
        "\n",
        "A Fourier transform (`tf.signal.fft`) converts a signal to its component frequencies, but loses all time information. In comparison, STFT (`tf.signal.stft`) splits the signal into windows of time and runs a Fourier transform on each window, preserving some time information, and returning a 2D tensor that you can run standard convolutions on.\n",
        "\n",
        "Create a utility function for converting waveforms to spectrograms:\n",
        "\n",
        "- The waveforms need to be of the same length, so that when you convert them to spectrograms, the results have similar dimensions. This can be done by simply zero-padding the audio clips that are shorter than one second (using `tf.zeros`).\n",
        "- When calling `tf.signal.stft`, choose the `frame_length` and `frame_step` parameters such that the generated spectrogram \"image\" is almost square. For more information on the STFT parameters choice, refer to [this Coursera video](https://www.coursera.org/lecture/audio-signal-processing/stft-2-tjEQe) on audio signal processing and STFT.\n",
        "- The STFT produces an array of complex numbers representing magnitude and phase. However, in this tutorial you'll only use the magnitude, which you can derive by applying `tf.abs` on the output of `tf.signal.stft`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4CK75DHz_OR"
      },
      "outputs": [],
      "source": [
        "def get_spectrogram(waveform):\n",
        "  # Convert the waveform to a spectrogram via a STFT.\n",
        "  spectrogram = tf.signal.stft(\n",
        "      waveform, frame_length=255, frame_step=128)\n",
        "  # Obtain the magnitude of the STFT.\n",
        "  spectrogram = tf.abs(spectrogram)\n",
        "  # Add a `channels` dimension, so that the spectrogram can be used\n",
        "  # as image-like input data with convolution layers (which expect\n",
        "  # shape (`batch_size`, `height`, `width`, `channels`).\n",
        "  spectrogram = spectrogram[..., tf.newaxis]\n",
        "  return spectrogram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rdPiPYJphs2"
      },
      "source": [
        "Next, start exploring the data. Print the shapes of one example's tensorized waveform and the corresponding spectrogram, and play the original audio:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Mu6Y7Yz3C-V"
      },
      "outputs": [],
      "source": [
        "for i in range(3):\n",
        "  label = label_names[example_labels[i]]\n",
        "  waveform = example_audio[i]\n",
        "  spectrogram = get_spectrogram(waveform)\n",
        "\n",
        "  print('Label:', label)\n",
        "  print('Waveform shape:', waveform.shape)\n",
        "  print('Spectrogram shape:', spectrogram.shape)\n",
        "  print('Audio playback')\n",
        "  display.display(display.Audio(waveform, rate=16000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnSuqyxJ1isF"
      },
      "source": [
        "Now, define a function for displaying a spectrogram:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e62jzb36-Jog"
      },
      "outputs": [],
      "source": [
        "def plot_spectrogram(spectrogram, ax):\n",
        "  if len(spectrogram.shape) > 2:\n",
        "    assert len(spectrogram.shape) == 3\n",
        "    spectrogram = np.squeeze(spectrogram, axis=-1)\n",
        "  # Convert the frequencies to log scale and transpose, so that the time is\n",
        "  # represented on the x-axis (columns).\n",
        "  # Add an epsilon to avoid taking a log of zero.\n",
        "  log_spec = np.log(spectrogram.T + np.finfo(float).eps)\n",
        "  height = log_spec.shape[0]\n",
        "  width = log_spec.shape[1]\n",
        "  X = np.linspace(0, np.size(spectrogram), num=width, dtype=int)\n",
        "  Y = range(height)\n",
        "  ax.pcolormesh(X, Y, log_spec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baa5c91e8603"
      },
      "source": [
        "Plot the example's waveform over time and the corresponding spectrogram (frequencies over time):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2_CikgY1tjv"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, figsize=(12, 8))\n",
        "timescale = np.arange(waveform.shape[0])\n",
        "axes[0].plot(timescale, waveform.numpy())\n",
        "axes[0].set_title('Waveform')\n",
        "axes[0].set_xlim([0, 16000])\n",
        "\n",
        "plot_spectrogram(spectrogram.numpy(), axes[1])\n",
        "axes[1].set_title('Spectrogram')\n",
        "plt.suptitle(label.title())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyYXjW07jCHA"
      },
      "source": [
        "Now, create spectrogram datasets from the audio datasets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAD0LpkgqtQo"
      },
      "outputs": [],
      "source": [
        "def make_spec_ds(ds):\n",
        "  return ds.map(\n",
        "      map_func=lambda audio,label: (get_spectrogram(audio), label),\n",
        "      num_parallel_calls=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEVb_oK0oBLQ"
      },
      "outputs": [],
      "source": [
        "train_spectrogram_ds = make_spec_ds(train_ds)\n",
        "val_spectrogram_ds = make_spec_ds(val_ds)\n",
        "test_spectrogram_ds = make_spec_ds(test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gQpAAgMnyDi"
      },
      "source": [
        "Examine the spectrograms for different examples of the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaM2q5aGis-d"
      },
      "outputs": [],
      "source": [
        "for example_spectrograms, example_spect_labels in train_spectrogram_ds.take(1):\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "QUbHfTuon4iF",
        "outputId": "15e36b70-c458-456c-c819-77f6d56fbf82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 699
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'example_spectrograms' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-70a04e17911d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mplot_spectrogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_spectrograms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexample_spect_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'example_spectrograms' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x900 with 9 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "rows = 3\n",
        "cols = 3\n",
        "n = rows*cols\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(16, 9))\n",
        "\n",
        "for i in range(n):\n",
        "    r = i // cols\n",
        "    c = i % cols\n",
        "    ax = axes[r][c]\n",
        "    plot_spectrogram(example_spectrograms[i].numpy(), ax)\n",
        "    ax.set_title(label_names[example_spect_labels[i].numpy()])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5KdY8IF8rkt"
      },
      "source": [
        "## Build and train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GS1uIh6F_TN9"
      },
      "source": [
        "Add `Dataset.cache` and `Dataset.prefetch` operations to reduce read latency while training the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdZ6M-F5_QzY"
      },
      "outputs": [],
      "source": [
        "train_spectrogram_ds = train_spectrogram_ds.cache().shuffle(10000).prefetch(tf.data.AUTOTUNE)\n",
        "val_spectrogram_ds = val_spectrogram_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
        "test_spectrogram_ds = test_spectrogram_ds.cache().prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwHkKCQQb5oW"
      },
      "source": [
        "For the model, you'll use a simple convolutional neural network (CNN), since you have transformed the audio files into spectrogram images.\n",
        "\n",
        "Your `tf.keras.Sequential` model will use the following Keras preprocessing layers:\n",
        "\n",
        "- `tf.keras.layers.Resizing`: to downsample the input to enable the model to train faster.\n",
        "- `tf.keras.layers.Normalization`: to normalize each pixel in the image based on its mean and standard deviation.\n",
        "\n",
        "For the `Normalization` layer, its `adapt` method would first need to be called on the training data in order to compute aggregate statistics (that is, the mean and the standard deviation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALYz7PFCHblP"
      },
      "outputs": [],
      "source": [
        "input_shape = example_spectrograms.shape[1:]\n",
        "print('Input shape:', input_shape)\n",
        "num_labels = len(label_names)\n",
        "\n",
        "# Instantiate the `tf.keras.layers.Normalization` layer.\n",
        "norm_layer = layers.Normalization()\n",
        "# Fit the state of the layer to the spectrograms\n",
        "# with `Normalization.adapt`.\n",
        "norm_layer.adapt(data=train_spectrogram_ds.map(map_func=lambda spec, label: spec))\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=input_shape),\n",
        "    # Downsample the input.\n",
        "    layers.Resizing(32, 32),\n",
        "    # Normalize.\n",
        "    norm_layer,\n",
        "    layers.Conv2D(32, 3, activation='relu'),\n",
        "    layers.Conv2D(64, 3, activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Dropout(0.25),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(num_labels),\n",
        "])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de52e5afa2f3"
      },
      "source": [
        "Configure the Keras model with the Adam optimizer and the cross-entropy loss:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFjj7-EmsTD-"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f42b9e3a4705"
      },
      "source": [
        "Train the model over 10 epochs for demonstration purposes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttioPJVMcGtq"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 10\n",
        "history = model.fit(\n",
        "    train_spectrogram_ds,\n",
        "    validation_data=val_spectrogram_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=2),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjpCDeQ4mUfS"
      },
      "source": [
        "Let's plot the training and validation loss curves to check how your model has improved during training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzhipg3Gu2AY"
      },
      "outputs": [],
      "source": [
        "metrics = history.history\n",
        "plt.figure(figsize=(16,6))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history.epoch, metrics['loss'], metrics['val_loss'])\n",
        "plt.legend(['loss', 'val_loss'])\n",
        "plt.ylim([0, max(plt.ylim())])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss [CrossEntropy]')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history.epoch, 100*np.array(metrics['accuracy']), 100*np.array(metrics['val_accuracy']))\n",
        "plt.legend(['accuracy', 'val_accuracy'])\n",
        "plt.ylim([0, 100])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy [%]')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZTt3kO3mfm4"
      },
      "source": [
        "## Evaluate the model performance\n",
        "\n",
        "Run the model on the test set and check the model's performance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FapuRT_SsWGQ"
      },
      "outputs": [],
      "source": [
        "model.evaluate(test_spectrogram_ds, return_dict=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "en9Znt1NOabH"
      },
      "source": [
        "### Display a confusion matrix\n",
        "\n",
        "Use a [confusion matrix](https://developers.google.com/machine-learning/glossary#confusion-matrix) to check how well the model did classifying each of the commands in the test set:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Y6vmWWQuuT1"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(test_spectrogram_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6F0il82u7lW"
      },
      "outputs": [],
      "source": [
        "y_pred = tf.argmax(y_pred, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHSNoBYLvX81"
      },
      "outputs": [],
      "source": [
        "y_true = tf.concat(list(test_spectrogram_ds.map(lambda s,lab: lab)), axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvoSAOiXU3lL"
      },
      "outputs": [],
      "source": [
        "confusion_mtx = tf.math.confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(confusion_mtx,\n",
        "            xticklabels=label_names,\n",
        "            yticklabels=label_names,\n",
        "            annot=True, fmt='g')\n",
        "plt.xlabel('Prediction')\n",
        "plt.ylabel('Label')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQGi_mzPcLvl"
      },
      "source": [
        "## Run inference on an audio file\n",
        "\n",
        "Finally, verify the model's prediction output using an input audio file of someone saying \"no\". How well does your model perform?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRxauKMdhofU"
      },
      "outputs": [],
      "source": [
        "x = data_dir/'no/01bb6a2a_nohash_0.wav'\n",
        "x = tf.io.read_file(str(x))\n",
        "x, sample_rate = tf.audio.decode_wav(x, desired_channels=1, desired_samples=16000,)\n",
        "x = tf.squeeze(x, axis=-1)\n",
        "waveform = x\n",
        "x = get_spectrogram(x)\n",
        "x = x[tf.newaxis,...]\n",
        "\n",
        "prediction = model(x)\n",
        "x_labels = ['no', 'yes', 'down', 'go', 'left', 'up', 'right', 'stop']\n",
        "plt.bar(x_labels, tf.nn.softmax(prediction[0]))\n",
        "plt.title('No')\n",
        "plt.show()\n",
        "\n",
        "display.display(display.Audio(waveform, rate=16000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgWICqdqQNaQ"
      },
      "source": [
        "As the output suggests, your model should have recognized the audio command as \"no\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1icqlM3ISW0"
      },
      "source": [
        "## Export the model with preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7HX-MjgIbji"
      },
      "source": [
        "The model's not very easy to use if you have to apply those preprocessing steps before passing data to the model for inference. So build an end-to-end version:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lIeXdWjIbDE"
      },
      "outputs": [],
      "source": [
        "class ExportModel(tf.Module):\n",
        "  def __init__(self, model):\n",
        "    self.model = model\n",
        "\n",
        "    # Accept either a string-filename or a batch of waveforms.\n",
        "    # You could add additional signatures for a single wave, or a ragged-batch.\n",
        "    self.__call__.get_concrete_function(\n",
        "        x=tf.TensorSpec(shape=(), dtype=tf.string))\n",
        "    self.__call__.get_concrete_function(\n",
        "       x=tf.TensorSpec(shape=[None, 16000], dtype=tf.float32))\n",
        "\n",
        "\n",
        "  @tf.function\n",
        "  def __call__(self, x):\n",
        "    # If they pass a string, load the file and decode it.\n",
        "    if x.dtype == tf.string:\n",
        "      x = tf.io.read_file(x)\n",
        "      x, _ = tf.audio.decode_wav(x, desired_channels=1, desired_samples=16000,)\n",
        "      x = tf.squeeze(x, axis=-1)\n",
        "      x = x[tf.newaxis, :]\n",
        "\n",
        "    x = get_spectrogram(x)\n",
        "    result = self.model(x, training=False)\n",
        "\n",
        "    class_ids = tf.argmax(result, axis=-1)\n",
        "    class_names = tf.gather(label_names, class_ids)\n",
        "    return {'predictions':result,\n",
        "            'class_ids': class_ids,\n",
        "            'class_names': class_names}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtZBmUiB9HGY"
      },
      "source": [
        "Test run the \"export\" model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1_8TYaCIRue"
      },
      "outputs": [],
      "source": [
        "export = ExportModel(model)\n",
        "export(tf.constant(str(data_dir/'no/01bb6a2a_nohash_0.wav')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1J6Iuz829Cxo"
      },
      "source": [
        "Save and reload the model, the reloaded model gives identical output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTAg4vsn3oEb"
      },
      "outputs": [],
      "source": [
        "tf.saved_model.save(export, \"saved\")\n",
        "imported = tf.saved_model.load(\"saved\")\n",
        "imported(waveform[tf.newaxis, :])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3jF933m9z1J"
      },
      "source": [
        "## Next steps\n",
        "\n",
        "This tutorial demonstrated how to carry out simple audio classification/automatic speech recognition using a convolutional neural network with TensorFlow and Python. To learn more, consider the following resources:\n",
        "\n",
        "- The [Sound classification with YAMNet](https://www.tensorflow.org/hub/tutorials/yamnet) tutorial shows how to use transfer learning for audio classification.\n",
        "- The notebooks from [Kaggle's TensorFlow speech recognition challenge](https://www.kaggle.com/c/tensorflow-speech-recognition-challenge/overview).\n",
        "- The\n",
        "[TensorFlow.js - Audio recognition using transfer learning codelab](https://codelabs.developers.google.com/codelabs/tensorflowjs-audio-codelab/index.html#0) teaches how to build your own interactive web app for audio classification.\n",
        "- [A tutorial on deep learning for music information retrieval](https://arxiv.org/abs/1709.04396) (Choi et al., 2017) on arXiv.\n",
        "- TensorFlow also has additional support for [audio data preparation and augmentation](https://www.tensorflow.org/io/tutorials/audio) to help with your own audio-based projects.\n",
        "- Consider using the [librosa](https://librosa.org/) library for music and audio analysis."
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "simple_audio.ipynb",
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}